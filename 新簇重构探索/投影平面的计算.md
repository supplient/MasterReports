#! https://zhuanlan.zhihu.com/p/388593109

仅为我个人备忘使用。

# 问题：寻找投影平面使得投影点尽可能紧凑

# 一、输入&问题
设点集$S_x=\{x_1, x_2, \dots, x_n\}$，其中$x_i = \begin{bmatrix}\alpha_1^i \\ \alpha_2^i \\ \vdots \\ \alpha_k^i \end{bmatrix}$

设向量$X=\begin{bmatrix}x_1\ x_2\ \dots\ x_n\end{bmatrix}, \alpha_j = \begin{bmatrix}\alpha_j^1 \ \alpha_j^2\ \dots\ \alpha_j^n\end{bmatrix}$

也就是说$\alpha^i_j$为点$i$的第$j$维，$\alpha_j$为由所有点第$j$维构成的行向量。

假设$\overline{x} = \sum_{i=1}^nx_i = 0$，即点集$S_x$的均值点为坐标原点。

我们的目标是在这个k维空间里找到一个t维“平面”（$t<k$），将所有点投影到这个平面上后，投影点尽可能密集。

# 二、分析&目标
设投影后的点构成点集$S_y = \{y_1, y_2, \dots, y_n\}$，其中$y_i = \begin{bmatrix}\beta_1^i \\ \beta_2^i \\ \cdots \\ \beta_t^i\end{bmatrix}$

设向量$Y=\begin{bmatrix}y_1\ y_2\ \dots\ y_n\end{bmatrix}, \beta_j = \begin{bmatrix}\beta_j^1 \ \beta_j^2\ \dots\ \beta_j^n\end{bmatrix}$

设t维平面的基向量为$p_1, p_2, \dots, p_t$，其中$p_i$为k维列向量，则有由k维空间的基到t维平面的基的过渡矩阵$P=\begin{bmatrix}p_1\ p_2\ \dots\ p_t\end{bmatrix}$，满足$Py_i = x_i$。

则有

$$
\begin{aligned}
	Y
	&=[y_1\ y_2\ \dots\ y_n] \\
	&=[P^{-1}x_1\ P^{-1}x_2\ \dots\ P^{-1}x_n] \\
	&=P^{-1}[x_1\ x_2\ \dots\ x_n] \\
	&=P^{-1}X
\end{aligned}
$$

因为$P$为绕坐标原点的旋转矩阵（任何过渡矩阵都是绕坐标原点的旋转矩阵），所以$\overline{y} = \overline{x} = 0$，即点集$S_y$的均值点也为坐标原点。

我们要求投影平面，就是要求这个P，下面讨论$P$应满足的条件。

## 目标1：$p_i$线性无关
因为我们希望投影后确确实实是在一个t维“平面”上，而不是在t-1维“线段”之类的上面。所以变换后的各维应该要线性无关。
* 例如，想象三维空间投影到二维平面上，如果构成二维平面的两个基向量相互平行的话，那其实是定义不出这个二维平面的，只能根据一个点和一个方向定义出一个直线。所以我们希望这两个基向量能线性无关。（事实上，线性无关是包含在[基向量的定义](https://en.wikipedia.org/wiki/Basis_(linear_algebra))中的）

所以t维平面的基应由t个线性无关的向量构成，也就是$p_1, p_2, \dots, p_t$两两线性无关。

## 目标2：$Cov(\beta_i, \beta_i)$尽可能小
我们希望投影后的点尽可能“密集”。密集的意思就是各点相互之间离得尽量近一些，我们可以使用方差来描述这个概念。一组数据的方差指的是这组数据的离散程度，例如一组点的x坐标的方差描述的是它们在x轴上有多分散。

所以要想让投影后的点尽可能密集，就是要让它们在t各维度上的方差都尽可能小。

也就是

$$
\min_{j=1,2,\dots, t} Cov(\beta_j, \beta_j) = \frac{1}{n-1}\sum_{i=1}^n(\beta_j^i-\overline{\beta_j})^2
$$

其中$\overline{\beta_j} = \frac{1}{n}\sum_{i=1}^n\beta_j^i = 0$，为各点在第j维上的均值。


## 目标3：$i\neq j, Cov(\beta_i, \beta_j)=0$
$$
\begin{aligned}
	由 PY&=X \\
	得 P\begin{bmatrix}
		\beta_1 \\ \beta_2 \\ \vdots \\ \beta_t
	\end{bmatrix} &= X \\
	\Rightarrow \sum_{i=1}^t p_i\beta_i &= X
\end{aligned}
$$

考虑$t=2$的情况，也就是投影到二维平面上时：

$$
	p_1\beta_1 + p_2\beta_2 = X
$$

若$\beta_2 = h\beta_1 + d$，即两者呈线性关系，则：

$$
	(p_1+hp_2)\beta_1 = X-p_2d
$$

注意到此时虽然$p_1, p_2$线性无关，但是所有点其实都沿$p1+hp_2$这个方向分布（等号右边的$X-p_2d$为定值），也就是说原本的二维平面此时退化成了一条一维直线。

而我们希望的是投影到一个二维平面上，所以我们希望避免这种退化，也就是希望$\beta_1, \beta_2$不能够相互线性表出，这两个维度上的样本值不具有线性相关性，使用皮尔逊相关系数来表示这一性质（注意，仅仅只是$\beta_2 \neq h\beta_1 + d$是不够的，因为可能从里面去除几个点后，剩下的点又退化成直线了，我们是希望所有点都不要退化成直线）：

$$
\rho_{\beta_1, \beta_2} = \frac{Cov(\beta_1, \beta_2)}{\sigma_{\beta_1}\sigma_{\beta_2}} = 0
$$

即：

$$
	Cov(\beta_1, \beta_2) = 0
$$

可以简单泛化到t维平面的情况，也就是当$i\neq j$时：

$$
	Cov(\beta_i, \beta_j) = 0
$$


# 三、推导&推论
## 推论1：协方差矩阵$Y_c$为对角矩阵
以点集$S_y$为样本集，以$\beta_1, \beta_2, \dots, \beta_t$为随机变量，设其协方差矩阵为$Y_c$：

$$
\begin{aligned}
	Y_c = \begin{bmatrix}
		Cov(\beta_i, \beta_j)
	\end{bmatrix}, i控制行,j控制列
\end{aligned}
$$

则若要满足目标3：当$i\neq j$，$Cov(\beta_i, \beta_j)=0$，$Y_c$就需要是一个对角矩阵：它只有主对角线上元素可以非零。


## 推导1：$Y_c = P^{-1} X_c P$
$$
\begin{aligned}
	Y_c 
	&= \begin{bmatrix}
			Cov(\beta_i, \beta_j)
		\end{bmatrix}, i控制行,j控制列 \\
	&(\because \overline{\beta_i}=0)\\
	&= \begin{bmatrix}
			\frac{1}{n-1}\beta_i\cdot\beta_j
		\end{bmatrix} \\
	&= \frac{1}{n-1} 
		\begin{bmatrix}
			\beta_1 \\
			\beta_2 \\
			\vdots \\
			\beta_n \\
		\end{bmatrix}
		\begin{bmatrix}
			\beta_1 \ 
			\beta_2 \ 
			\cdots \ 
			\beta_n
		\end{bmatrix} \\ 
	&= \frac{1}{n-1} YY^T
		
\end{aligned}
$$

同理，X的协方差矩阵$X_c=\frac{1}{n-1}XX^T$

然后，考虑过渡矩阵P，其满足$PY=X$，因此：

$$ 
\begin{aligned}
	Y_c 
	&= \frac{1}{n-1}YY^T \\
	&= \frac{1}{n-1}(P^{-1}X)(P^{-1}X)^T \\
	&(\text{P is orthogonal matrix, so }P^{-1}=P^T) \\
	&= \frac{1}{n-1}P^{-1}XX^TP \\
	&= P^{-1}(\frac{1}{n-1}XX^T)P \\
	&= P^{-1}X_cP \\
\end{aligned}
$$

故而要让$Y_c$为对角矩阵，就是要让$X_c$对角化。

## 推导2：$X_c$相似对角化
注意到$X_c$为实对称矩阵，故而其必定可以相似对角化。

设$X_c$的特征值从小到大为$\lambda_1, \lambda_2, \cdots, \lambda_k$（若存在多重特征值，则分开写成多个$\lambda$），对应的**单位**特征向量（列向量）为$v_1, v_2, \cdots, v_k$。并设$V = \begin{bmatrix}v_1\ v_2\ \cdots\ v_k\end{bmatrix}$，$\Sigma = diag(\lambda_1, \lambda_2, \cdots, \lambda_k)$，则有：

$$
	V^{-1}X_cV = \Sigma
$$

## 推论2：$P$等于V的左上$k\times t$子块，$Y_c$等于$\Sigma$的左上$t\times t$子块
首先检查目标1，我们需要$p_i$线性无关。而$v_i$为$X_c$的特征向量，所以必定两两线性无关，故而满足目标1。

然后检查目标2，我们希望$Y_c$对角线上的值尽可能小。因为我们取的是$\Sigma$的左上角子块，而特征值按照从小到大的顺序排列，所以$Y_c$主对角线上的值会尽量得小。
TODO:关于这一点，我并没有严格证明，也没有逻辑推导，只是感觉差不多。

最后检查目标3，因为$\Sigma$是对角矩阵，所以其左上角子块也为对角矩阵，所以$Y_c$为对角矩阵。

# 四、结论
若$X$的均值点为原点，则通过相似对角化$X$的协方差矩阵$X_c=\frac{1}{n-1}XX^T$，得到$V^{-1}X_cV = \Sigma$，随后使$P$为$V$的左上角$k \times t$子块，就能得到过渡矩阵$P$，利用它就能变换得到投影到t维“平面”上的点$Y=P^{-1}X$：在该投影“平面”上，投影点尽可能地紧凑。

* 相似对角化$X_c$时，因为它为实对称矩阵，故而必定为Hermitian Matrix，可以直接应用eigen里的SelfAdjointEigenSolver来计算。
* 其他的计算就都只是矩阵乘法了。